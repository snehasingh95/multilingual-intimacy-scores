{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # quantifying intimacy score\n",
    "\n",
    "# # Import generic wrappers\n",
    "# def calculate_intimacy_score(inputs, label=None):\n",
    "#     # Define the model repo\n",
    "#     model_name = \"pedropei/question-intimacy\" \n",
    "\n",
    "\n",
    "#     # Download pytorch model\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "#     # list of strings should be pretokenized\n",
    "#     # add_prefix_space needed for pretokenized\n",
    "    \n",
    "#     t = tokenizer(inputs, return_tensors=\"pt\", is_split_into_words=True,\n",
    "#                  padding=True, truncation=True, max_length=50, add_special_tokens=True)\n",
    "#     if label:\n",
    "#         labels = torch.tensor(label).unsqueeze(0)\n",
    "#         # Model apply\n",
    "#         output = model(**t, labels=labels)\n",
    "    \n",
    "#     else:\n",
    "#         output = model(**t)\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "\n",
    "# inputs_fr = [\"Quelle est la question que vous détestez qu'on vous pose ?\", \n",
    "#              \"Quelle est l'importance de l'aérodynamique dans l'espace ?\", \n",
    "#              \"Quelle est la meilleure façon de leur faire avoir des analsecks ?\", \n",
    "#              \"Qu'est-ce que l'amour à 11 ans ?\"]\n",
    "\n",
    "# labels = [0.517, -0.421, 0.4999, 0.514]\n",
    "\n",
    "# inp_en = translation(inputs_fr)\n",
    "# print(inp_en)\n",
    "\n",
    "# score = calculate_intimacy_score(inp_en, labels)\n",
    "# print(score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# language translation\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "# language translation\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer \n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer \n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def translation(inputs_fr):\n",
    "    model_checkpoint = \"Helsinki-NLP/opus-mt-fr-en\"\n",
    "    translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "\n",
    "    inputs = []\n",
    "    for i_fr in tqdm(inputs_fr):\n",
    "        inp = translator(i_fr)\n",
    "\n",
    "        inp = inp[0]['translation_text']\n",
    "\n",
    "        inputs.append(inp)\n",
    "    \n",
    "    return inputs\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# quantifying intimacy score\n",
    "# Import generic wrappers\n",
    "from scipy import stats\n",
    "import torch\n",
    "\n",
    "def predict(inputs, labels, model, tokenizer):\n",
    "    # add_prefix_space needed for pretokenized\n",
    "    \n",
    "    predictions = []\n",
    "    loss = [] if labels is not None else None\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        t = tokenizer([inputs[i]], return_tensors=\"pt\", is_split_into_words=True,\n",
    "                 padding=True, truncation=True, max_length=50, add_special_tokens=True)\n",
    "        if labels:\n",
    "            label = torch.tensor(labels[i]).unsqueeze(0)\n",
    "            output = model(**t, labels=label)\n",
    "            \n",
    "            loss.append(output.loss.detach().numpy())\n",
    "        else:\n",
    "            output = model(**t)\n",
    "            \n",
    "        predictions.append(torch.flatten(output.logits.data).numpy()[0])\n",
    "    return predictions, loss\n",
    "        \n",
    "    \n",
    "\n",
    "def get_metrics(inputs, labels, model, tokenizer):\n",
    "    predictions, loss = predict(inputs, labels, model, tokenizer)\n",
    "    return np.mean(loss),stats.pearsonr(predictions,labels)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "inputs_fr = [\"Quelle est la question que vous détestez qu'on vous pose ?\", \n",
    "             \"Quelle est l'importance de l'aérodynamique dans l'espace ?\", \n",
    "             \"Quelle est la meilleure façon de leur faire avoir des analsecks ?\", \n",
    "             \"Qu'est-ce que l'amour à 11 ans ?\"]\n",
    "\n",
    "labels = [0.517, -0.421, 0.4999, 0.514]\n",
    "\n",
    "inp_en = translation(inputs_fr)\n",
    "print(inp_en)\n",
    "\n",
    "model_name = \"pedropei/question-intimacy\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "loss, pearsonr = get_metrics(inp_en, labels, model, tokenizer)\n",
    "print('loss:',loss, 'pearsonr:',pearsonr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.11it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"What's the question you hate being asked?\", 'What is the importance of aerodynamics in space?', \"What's the best way to make them have analsecks?\", 'What is 11-year-old love?']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at pedropei/question-intimacy were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 0.029354256 pearsonr: 0.9718837357940412\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import import_ipynb\n",
    "from load_data import load_data, getLanguageData, k_folds, k_fold_split, shuffle_data\n",
    "\n",
    "k=10\n",
    "\n",
    "raw_data = load_data(\"data/multi_language_data/\", \"train_normalized.csv\")['train']\n",
    "\n",
    "#English\n",
    "print('English')\n",
    "en_text, en_label = getLanguageData(raw_data, 'English')\n",
    "\n",
    "#French\n",
    "print('\\nFrench')\n",
    "fr_text, fr_label = getLanguageData(raw_data, 'French')\n",
    "fr_text_folds,fr_label_folds = k_folds(k, [fr_text], [fr_label])\n",
    "\n",
    "#translation\n",
    "text = en_text\n",
    "label = en_label\n",
    "print(\"=======================Translating French to English=============================\")\n",
    "for i in range(k):\n",
    "    translated_text = translation(fr_text_folds[i])\n",
    "    text.extend(translated_text)\n",
    "    label.extend(fr_label_folds[i])\n",
    "print('text:',len(text))\n",
    "\n",
    "# label = en_label\n",
    "# label.extend(fr_label)\n",
    "print('Train_label:',len(label))\n",
    "\n",
    "#Shuffle \n",
    "text, label = shuffle_data(text, label)\n",
    "\n",
    "model_name = \"pedropei/question-intimacy\" \n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "print('======================HELSINKI TRANSLATION MODEL FULL PIPELINE===============')\n",
    "print(\"Predicting Intimacy Scores\")\n",
    "\n",
    "\n",
    "# score = calculate_intimacy_score(text, label)\n",
    "# print(score)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing Jupyter notebook from load_data.ipynb\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration data-9e64f10bc604987e\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset csv/data to /home/siri/.cache/huggingface/datasets/csv/data-9e64f10bc604987e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b605948b8445d7bb724be2f62bb148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d77f63f32e4d7ebbb2b5dc1a9f5144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/scratch/user/siri/NLP/nlp_env/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py:714: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset csv downloaded and prepared to /home/siri/.cache/huggingface/datasets/csv/data-9e64f10bc604987e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5281c031f344a77b74a42d39f8229bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "English\n",
      "Folding...\n",
      "Fold 1 : 793 data\n",
      "Fold 2 : 794 data\n",
      "Split 1 - Training Data: 793 - Validation Data: 793\n",
      "Split 2 - Training Data: 794 - Validation Data: 794\n",
      "\n",
      "French\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration data-9e64f10bc604987e\n",
      "Found cached dataset csv (/home/siri/.cache/huggingface/datasets/csv/data-9e64f10bc604987e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folding...\n",
      "Fold 1 : 794 data\n",
      "Fold 2 : 794 data\n",
      "Split 1 - Training Data: 794 - Validation Data: 794\n",
      "Split 2 - Training Data: 794 - Validation Data: 794\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b74a7da5054f0bab32ae8a84a09983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "English\n",
      "\n",
      "French\n",
      "Folding...\n",
      "Fold 1 : 158 data\n",
      "Fold 2 : 158 data\n",
      "Fold 3 : 158 data\n",
      "Fold 4 : 158 data\n",
      "Fold 5 : 158 data\n",
      "Fold 6 : 158 data\n",
      "Fold 7 : 158 data\n",
      "Fold 8 : 158 data\n",
      "Fold 9 : 158 data\n",
      "Fold 10 : 166 data\n",
      "=======================Translating French to English=============================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/scratch/user/siri/NLP/transformers/src/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "100%|██████████| 158/158 [03:28<00:00,  1.32s/it]\n",
      "100%|██████████| 158/158 [03:16<00:00,  1.24s/it]\n",
      "100%|██████████| 158/158 [03:40<00:00,  1.39s/it]\n",
      "100%|██████████| 158/158 [03:03<00:00,  1.16s/it]\n",
      "100%|██████████| 158/158 [03:31<00:00,  1.34s/it]\n",
      "100%|██████████| 158/158 [03:29<00:00,  1.33s/it]\n",
      "100%|██████████| 158/158 [03:33<00:00,  1.35s/it]\n",
      "100%|██████████| 158/158 [03:37<00:00,  1.38s/it]\n",
      "100%|██████████| 158/158 [04:07<00:00,  1.57s/it]\n",
      "100%|██████████| 166/166 [04:11<00:00,  1.51s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "text: 3175\n",
      "Train_label: 3175\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at pedropei/question-intimacy were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================HELSINKI TRANSLATION MODEL FULL PIPELINE===============\n",
      "Predicting Intimacy Scores\n",
      "loss: 0.029354256 pearsonr: 0.9718837357940412\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "loss, pearsonr = get_metrics(text,label, model, tokenizer)\n",
    "print('loss:',loss, 'pearsonr:',pearsonr)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 0.10862088 pearsonr: 0.4796358981817467\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}